{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f21807",
   "metadata": {},
   "source": [
    "# Offer ID Data Retrieval\n",
    "Input an offer ID to automatically fetch and process product data from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09e05515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(variable, filename):\n",
    "    \"\"\"Save a variable to JSON file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(variable, f, indent=2, default=str, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55d27b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to MongoDB\n",
      "üîó MongoDB client is ready for use\n",
      "üìã Available databases: ['markazmongodbprod']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Establish MongoDB Connection\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "def connect_to_mongodb():\n",
    "    \"\"\"\n",
    "    Establish connection to MongoDB\n",
    "    Returns MongoClient instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # MongoDB connection string - modify as needed\n",
    "        connection_string = \"mongodb://ahmadjanmongodb:ahmadjanmongodb2026@136.110.31.24:27017,136.110.48.82:27017,34.158.51.107:27017/markazmongodbprod?authSource=markazmongodbprod\"\n",
    "        \n",
    "        # If your MongoDB requires authentication, use this format:\n",
    "        # connection_string = \"mongodb://username:password@hostname:port/database_name\"\n",
    "        \n",
    "        client = MongoClient(connection_string)\n",
    "        \n",
    "        # Test the connection\n",
    "        client.admin.command('ping')\n",
    "        print(\"‚úÖ Successfully connected to MongoDB\")\n",
    "        return client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error connecting to MongoDB: {e}\")\n",
    "        print(\"üí° Make sure MongoDB is running and the connection string is correct\")\n",
    "        return None\n",
    "\n",
    "# Test the connection\n",
    "mongo_client = connect_to_mongodb()\n",
    "\n",
    "if mongo_client:\n",
    "    print(\"üîó MongoDB client is ready for use\")\n",
    "    print(\"üìã Available databases:\", mongo_client.list_database_names())\n",
    "else:\n",
    "    print(\"‚ùå MongoDB connection failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8c126f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching offer ID: 653479347253\n",
      "üîç Searching for offer ID: 653479347253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document found!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Fetch Data with Hardcoded Offer ID\n",
    "def fetch_dbResponse(offerId, database_name, collection_name):\n",
    "    \"\"\"Fetch product data from MongoDB collection using offer ID\"\"\"\n",
    "    if not mongo_client:\n",
    "        print(\"‚ùå MongoDB client not available. Run the connection cell first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Searching for offer ID: {offerId}\")\n",
    "        \n",
    "        db = mongo_client[database_name]\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        # Search for document\n",
    "        document = collection.find_one({\"offerId\": int(offerId)})\n",
    "        \n",
    "        if document:\n",
    "            print(f\"‚úÖ Document found!\")\n",
    "            return document\n",
    "        else:\n",
    "            print(f\"‚ùå No document found with offer ID: {offerId}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuration\n",
    "offerId = \"653479347253\"\n",
    "DATABASE_NAME = \"markazmongodbprod\"\n",
    "COLLECTION_NAME = \"productsV2\"\n",
    "\n",
    "print(f\"Fetching offer ID: {offerId}\")\n",
    "\n",
    "# Fetch and display the data\n",
    "# json that will be used further\n",
    "dbResponse = fetch_dbResponse(offerId, DATABASE_NAME, COLLECTION_NAME)\n",
    "# save_to_json(dbResponse, \"dbResponse.json\")\n",
    "# if dbResponse:\n",
    "#     print(f\"üìä Document size: {len(json.dumps(dbResponse, default=str))} characters\")\n",
    "    \n",
    "#     # Store for next steps\n",
    "#     globals()['current_dbResponse'] = dbResponse\n",
    "#     globals()['current_offerId'] = offerId\n",
    "    \n",
    "#     print(\"‚úÖ Data ready for processing\")\n",
    "# else:\n",
    "#     print(\"‚ùå Failed to retrieve document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b79ff",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa8f6455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed product data:\n",
      "{\n",
      "    \"id\": \"68f9e17b5dc1e9728a133c05\",\n",
      "    \"categories\": [\n",
      "        {\n",
      "            \"categoryId\": 10166,\n",
      "            \"categoryName\": \"Women's Clothing\"\n",
      "        },\n",
      "        {\n",
      "            \"categoryId\": 127386001,\n",
      "            \"categoryName\": \"women's sweater\"\n",
      "        }\n",
      "    ],\n",
      "    \"name\": \"New Autumn and Winter Pullover Knitwear Women's Bottoming Shirt 2025Bf Loose Lazy Style Hong Kong Style Jacquard Quilted Knitwear Women\",\n",
      "    \"Product info\": {\n",
      "        \"length\": null,\n",
      "        \"weight\": null,\n",
      "        \"height\": null,\n",
      "        \"width\": null,\n",
      "        \"aiWeight\": null\n",
      "    },\n",
      "    \"skus\": [\n",
      "        {\n",
      "            \"skuId\": 4712603881561,\n",
      "            \"skuAttributes\": [\n",
      "                {\n",
      "                    \"attributeId\": 3216,\n",
      "                    \"attributeName\": \"Color\",\n",
      "                    \"value\": \"Beige + Orange\"\n",
      "                },\n",
      "                {\n",
      "                    \"attributeId\": 450,\n",
      "                    \"attributeName\": \"Size\",\n",
      "                    \"value\": \"All yards\"\n",
      "                }\n",
      "            ],\n",
      "            \"length\": null,\n",
      "            \"weight\": null,\n",
      "            \"height\": null,\n",
      "            \"width\": null,\n",
      "            \"aiWeight\": 0.487\n",
      "        },\n",
      "        {\n",
      "            \"skuId\": 4712603881563,\n",
      "            \"skuAttributes\": [\n",
      "                {\n",
      "                    \"attributeId\": 3216,\n",
      "                    \"attributeName\": \"Color\",\n",
      "                    \"value\": \"Gray + Wine Red\"\n",
      "                },\n",
      "                {\n",
      "                    \"attributeId\": 450,\n",
      "                    \"attributeName\": \"Size\",\n",
      "                    \"value\": \"All yards\"\n",
      "                }\n",
      "            ],\n",
      "            \"length\": null,\n",
      "            \"weight\": null,\n",
      "            \"height\": null,\n",
      "            \"width\": null,\n",
      "            \"aiWeight\": 0.499\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "‚úÖ Successfully saved processed data to 'preprocessed.json'\n",
      "Number of products processed: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def filter_product_data(product_json):\n",
    "    \"\"\"\n",
    "    Extracts shipping, ID, and Attribute information from a single product JSON object.\n",
    "    Iterates through 'productSkuInfos' to get details for every SKU.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 0. Safety Check\n",
    "    if not isinstance(product_json, dict):\n",
    "        return None\n",
    "\n",
    "    # --- 1. ID Handling ---\n",
    "    raw_id = product_json.get('_id')\n",
    "    product_id = raw_id.get('$oid') if isinstance(raw_id, dict) else raw_id\n",
    "\n",
    "    # --- 2. Extract General Product Info ---\n",
    "    # We grab the general shipping info from the first SKU item as a fallback/general reference\n",
    "    sku_infos = product_json.get('productSkuInfos', [])\n",
    "    if not isinstance(sku_infos, list):\n",
    "        sku_infos = []\n",
    "\n",
    "    first_sku_info = sku_infos[0] if sku_infos else {}\n",
    "    if not isinstance(first_sku_info, dict): \n",
    "        first_sku_info = {}\n",
    "\n",
    "    # This is the general container for the whole product\n",
    "    product_shipping_info = first_sku_info.get('productShippingInfo', {})\n",
    "    if not isinstance(product_shipping_info, dict):\n",
    "        product_shipping_info = {}\n",
    "\n",
    "    product_info = {\n",
    "        \"length\": product_shipping_info.get('length'),\n",
    "        \"weight\": product_shipping_info.get('weight'),\n",
    "        \"height\": product_shipping_info.get('height'),\n",
    "        \"width\":  product_shipping_info.get('width'),\n",
    "        \"aiWeight\": product_shipping_info.get('aiWeight')\n",
    "    }\n",
    "\n",
    "    # --- 3. Extract SKU Details & Attributes ---\n",
    "    formatted_skus = []\n",
    "\n",
    "    # We iterate through the main 'productSkuInfos' list to find attributes AND dimensions for each SKU\n",
    "    for info in sku_infos:\n",
    "        if not isinstance(info, dict):\n",
    "            continue\n",
    "\n",
    "        # A. Extract SKU ID (Handle {$numberLong: \"...\"} format if present)\n",
    "        raw_sku_id = info.get('skuId')\n",
    "        if isinstance(raw_sku_id, dict):\n",
    "            sku_id = raw_sku_id.get('$numberLong') # MongoDB specific format\n",
    "        else:\n",
    "            sku_id = raw_sku_id\n",
    "\n",
    "        # B. Extract Attributes\n",
    "        # This is the new field you requested\n",
    "        sku_attributes = info.get('skuAttributes', [])\n",
    "\n",
    "        # C. Extract Dimensions\n",
    "        # Inside each element of productSkuInfos, there is usually a 'skuShippingDetail' (singular) object\n",
    "        detail = info.get('skuShippingDetail', {})\n",
    "        if not isinstance(detail, dict):\n",
    "            detail = {}\n",
    "\n",
    "        sku_entry = {\n",
    "            \"skuId\": sku_id,\n",
    "            \"skuAttributes\": sku_attributes,  # <--- Added Field\n",
    "            \"length\": detail.get('length'),\n",
    "            \"weight\": detail.get('weight'),\n",
    "            \"height\": detail.get('height'),\n",
    "            \"width\":  detail.get('width'),\n",
    "            \"aiWeight\": detail.get('aiWeight') # May be None if not present in this sub-object\n",
    "        }\n",
    "        formatted_skus.append(sku_entry)\n",
    "\n",
    "    # --- 4. Construct Final Output ---\n",
    "    output_data = {\n",
    "        \"id\": product_id,\n",
    "        \"categories\": product_json.get('categories'),\n",
    "        \"name\": product_json.get('name'),\n",
    "        \"Product info\": product_info,\n",
    "        \"skus\": formatted_skus\n",
    "    }\n",
    "\n",
    "    return output_data\n",
    "\n",
    "# ==========================================\n",
    "# Process MongoDB Data  \n",
    "# ==========================================\n",
    "\n",
    "try:\n",
    "    # Handle both single product object and array of products\n",
    "    if isinstance(dbResponse, dict):\n",
    "        # Single product object\n",
    "        all_products = [dbResponse]\n",
    "    elif isinstance(dbResponse, list):\n",
    "        # Array of products\n",
    "        all_products = dbResponse\n",
    "    else:\n",
    "        raise ValueError(\"Invalid JSON structure\")\n",
    "\n",
    "    # Process all products\n",
    "    processed_results = [filter_product_data(p) for p in all_products if isinstance(p, dict)]\n",
    "\n",
    "    # Check if we have results\n",
    "    if processed_results:\n",
    "        # Print the first result to verify\n",
    "        print(\"Processed product data:\")\n",
    "        print(json.dumps(processed_results[0], indent=4, ensure_ascii=False, default=str))\n",
    "\n",
    "        # Save to file\n",
    "        output_filename = 'preprocessed.json'\n",
    "        with open(output_filename, 'w', encoding='utf-8') as out_f:\n",
    "            json.dump(processed_results, out_f, indent=4, ensure_ascii=False, default=str)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully saved processed data to '{output_filename}'\")\n",
    "        print(f\"Number of products processed: {len(processed_results)}\")\n",
    "        \n",
    "        # Store in variable for next step\n",
    "        globals()['processed_results'] = processed_results\n",
    "    else:\n",
    "        print(\"‚ùå No valid products found to process\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File 'singleproduct.json' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d20f9e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 1 products from preprocessed.json\n",
      "\n",
      "Summary:\n",
      "Total SKUs before: 2\n",
      "Total SKUs after: 2\n",
      "SKUs removed: 0\n",
      "\n",
      "‚úÖ Cleaned data stored in 'cleaned_products' variable\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "def remove_duplicate_skus(data):\n",
    "    \"\"\"\n",
    "    Remove duplicate SKUs when all SKUs in a product have identical \n",
    "    weight, length, height, width, and aiWeight properties\n",
    "    Note: SKUs with all null/None values are NOT considered identical\n",
    "    \"\"\"\n",
    "    processed_data = deepcopy(data)\n",
    "    \n",
    "    for product in processed_data:\n",
    "        if 'skus' not in product or len(product['skus']) <= 1:\n",
    "            continue\n",
    "            \n",
    "        skus = product['skus']\n",
    "        first_sku = skus[0]\n",
    "        \n",
    "        # Extract physical properties from first SKU\n",
    "        first_props = {\n",
    "            'weight': first_sku.get('weight'),\n",
    "            'length': first_sku.get('length'), \n",
    "            'height': first_sku.get('height'),\n",
    "            'width': first_sku.get('width'),\n",
    "            'aiWeight': first_sku.get('aiWeight')\n",
    "        }\n",
    "        \n",
    "        # Check if all properties are null/None - if so, skip duplicate removal\n",
    "        all_props_null = all(value is None for value in first_props.values())\n",
    "        if all_props_null:\n",
    "            print(f\"Product {product.get('name', product.get('id', 'Unknown'))}: \"\n",
    "                  f\"Skipping duplicate removal - all properties are null\")\n",
    "            continue\n",
    "        \n",
    "        # Check if all SKUs have identical physical properties\n",
    "        all_identical = True\n",
    "        for sku in skus[1:]:\n",
    "            sku_props = {\n",
    "                'weight': sku.get('weight'),\n",
    "                'length': sku.get('length'),\n",
    "                'height': sku.get('height'), \n",
    "                'width': sku.get('width'),\n",
    "                'aiWeight': sku.get('aiWeight')\n",
    "            }\n",
    "            \n",
    "            if sku_props != first_props:\n",
    "                all_identical = False\n",
    "                break\n",
    "        \n",
    "        # If all physical properties are identical, keep only the first SKU\n",
    "        if all_identical:\n",
    "            print(f\"Product {product.get('name', product.get('id', 'Unknown'))}: \"\n",
    "                  f\"Reduced from {len(skus)} to 1 SKU (identical physical properties)\")\n",
    "            product['skus'] = [first_sku]\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# ==========================================\n",
    "# DATA SOURCE OPTIONS - Comment out one option\n",
    "# ==========================================\n",
    "\n",
    "# Option 1: Use processed_results variable from previous cell\n",
    "# if 'processed_results' in globals() and processed_results:\n",
    "#     original_data = processed_results\n",
    "#     print(f\"üìä Using {len(original_data)} products from previous processing\")\n",
    "\n",
    "# Option 2: Load from preprocessed.json file (comment out if using Option 1)\n",
    "try:\n",
    "    with open('preprocessed.json', 'r', encoding='utf-8') as f:\n",
    "        original_data = json.load(f)\n",
    "    print(f\"üìÇ Loaded {len(original_data)} products from preprocessed.json\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File 'preprocessed.json' not found.\")\n",
    "    original_data = []\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading file: {e}\")\n",
    "    original_data = []\n",
    "\n",
    "# ==========================================\n",
    "# PROCESS DATA (if available)\n",
    "# ==========================================\n",
    "\n",
    "if 'original_data' in locals() and original_data:\n",
    "    # Process the data to remove duplicate SKUs\n",
    "    processed_data = remove_duplicate_skus(original_data)\n",
    "    \n",
    "    # Calculate total SKUs before and after\n",
    "    total_skus_before = sum(len(product.get('skus', [])) for product in original_data)\n",
    "    total_skus_after = sum(len(product.get('skus', [])) for product in processed_data)\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Total SKUs before: {total_skus_before}\")\n",
    "    print(f\"Total SKUs after: {total_skus_after}\")\n",
    "    print(f\"SKUs removed: {total_skus_before - total_skus_after}\")\n",
    "    \n",
    "    # Store cleaned data in variable\n",
    "    globals()['cleaned_products'] = processed_data\n",
    "    print(f\"\\n‚úÖ Cleaned data stored in 'cleaned_products' variable\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data source available. Either run the previous cell or uncomment the file loading option.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3260c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to cleaned_products.json\n"
     ]
    }
   ],
   "source": [
    "save_to_json(cleaned_products,\"cleaned_products.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b35c3d",
   "metadata": {},
   "source": [
    "## cluade api call to generate estimated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c968c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Any\n",
    "import anthropic\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging for token usage tracking\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('product_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e74fb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Claude API client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set your Claude API key here\n",
    "API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")  # Load from environment variable\n",
    "# Or uncomment the next line to input it securely\n",
    "# API_KEY = input(\"Enter your Anthropic API key: \").strip()\n",
    "\n",
    "# Initialize Claude client\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "# Token tracking variables\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "api_calls_count = 0\n",
    "\n",
    "print(\"‚úÖ Claude API client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10955e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ System prompt configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for E-commerce Logistics Auditor\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "<system_instructions>\n",
    "    <role_definition>\n",
    "        You are an expert E-commerce Logistics Auditor. Your goal is to clean, validate, and impute physical product data (dimensions and weight) from raw JSON listings. You utilize the synergy between product names, category hierarchies, and specific variant attributes to establish strict ground-truth physical baselines.\n",
    "    </role_definition>\n",
    "\n",
    "    <input_structure>\n",
    "        You will receive a JSON list where each item contains:\n",
    "        1. productTrans: Product name (Primary signal for object identification).\n",
    "        2. categories: A hierarchical list of categories (e.g., Digital -> Mobile Accessories -> Phone Case). Use this to narrow down the \"expected physical envelope\" (e.g., a phone case vs. a phone).\n",
    "        3. main_info: General product dimensions/weight (often missing, aggregated, or inaccurate).\n",
    "        4. skus: A list of variants. Each SKU contains:\n",
    "           - skuAttributes: Specifics like \"Color\", \"Model\", or \"Size\" (e.g., \"Samsung Z Fold 7\" vs \"iPhone 13 Mini\").\n",
    "           - Dimensions/Weight data (scattered quality).\n",
    "    </input_structure>\n",
    "\n",
    "    <reasoning_process>\n",
    "        For each product in the input list, you must follow this strict 4-step logic:\n",
    "\n",
    "        STEP 1: CATEGORY & PRODUCT PROFILING (The \"Envelope\" Check)\n",
    "        - Analyze Hierarchy: Read the 'categories' list from broadest to most specific to set physical boundaries.\n",
    "          (Example: If category is \"Mobile Phone Protective Cover\", the object MUST be small (approx 10-20cm) and light (20g-100g).)\n",
    "        - Refine with Name: Use 'productTrans' to confirm the item type and catch \"accessory vs. device\" errors.\n",
    "          (Distinction: Ensure you are not confusing a \"Case for iPad\" (Light) with an \"iPad\" (Heavy).)\n",
    "        - Set Baselines: Establish a \"Valid Range\" for this specific category (e.g., \"Max valid weight is 200g. Any value like 3kg is a Unit Error\").\n",
    "\n",
    "        STEP 2: SKU DIFFERENTIATION ANALYSIS\n",
    "        - Scan Attributes: Analyze 'skuAttributes' to determine if variants *physically* differ.\n",
    "          - Cosmetic Attributes: \"Color\", \"Pattern\" -> These do NOT change dimensions/weight significantly. Treat these SKUs as physically identical.\n",
    "          - Physical Attributes: \"Applicable Model\" (e.g., S25 vs S25 Ultra), \"Size\", \"Capacity\" -> These DO change dimensions/weight. You must allow for variance here.\n",
    "        - Cluster Data: Group SKUs by their physical attributes. If \"Model A\" SKUs average 50g and \"Model B\" SKUs average 70g, preserve this difference.\n",
    "\n",
    "        STEP 3: UNIT PREDICTION & GLOBAL CLEANING\n",
    "        - Scan Data: Look at 'main_info' and SKU data collectively.\n",
    "        - Predict Units:\n",
    "          - If values are 2.34, 5.1 for a phone case: Is it Meters (too big)? Inches (possible)? CM (most likely)?\n",
    "          - If weight is 0.05: Is it Grams (too light)? Kg (50g, likely)?\n",
    "        - Flag Outliers: Detect values that violate the \"Valid Range\" established in Step 1.\n",
    "\n",
    "        STEP 4: FINAL IMPUTATION & STANDARDIZATION\n",
    "        - Iterate SKUs:\n",
    "          - Missing Values: If null/0, impute using the average of valid SKUs *within the same attribute cluster* (e.g., use other \"Z Fold 7\" weights for a missing \"Z Fold 7\" weight). If no cluster match exists, use the global product average.\n",
    "          - Outlier Correction: Replace impossible values (e.g., \"3kg\" for a case) with the calculated baseline.\n",
    "          - Standardization: Convert EVERYTHING to Centimeters (cm) and Grams (g).\n",
    "    </reasoning_process>\n",
    "    <output_rules>\n",
    "            Return a JSON List of objects (one object per product processed).\n",
    "            - Do NOT include markdown formatting (like ```json).\n",
    "            - Output strict JSON only.\n",
    "\n",
    "            Output JSON Structure:\n",
    "            [\n",
    "                {\n",
    "                    \"skus\": [\n",
    "                        {\n",
    "                            \"skuId\": \"String (Exactly as found in input)\",\n",
    "                            \"length_cm\": Float,\n",
    "                            \"width_cm\": Float,\n",
    "                            \"height_cm\": Float,\n",
    "                            \"weight_g\": Float\n",
    "                        },\n",
    "                        {\n",
    "                            \"skuId\": \"String\",\n",
    "                            \"length_cm\": Float,\n",
    "                            \"width_cm\": Float,\n",
    "                            \"height_cm\": Float,\n",
    "                            \"weight_g\": Float\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        </output_rules>\n",
    "</system_instructions>\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ System prompt configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "886144a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processing functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def prepare_product_data(products: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Prepare product data in the expected format for the API\"\"\"\n",
    "    prepared_data = []\n",
    "    \n",
    "    for product in products:\n",
    "        # Extract the key fields, handling different possible field names\n",
    "        product_name = product.get('name') or product.get('name', 'Unknown Product')\n",
    "        main_info = product.get('Product info', product.get('main_info', {}))\n",
    "        skus = product.get('skus', [])\n",
    "        categories = product.get('categories', [])\n",
    "        \n",
    "        # Prepare the product in the expected format\n",
    "        prepared_product = {\n",
    "            'name': product_name,\n",
    "            'main_info': main_info,\n",
    "            'skus': skus,\n",
    "            'categories': categories\n",
    "        }\n",
    "        \n",
    "        prepared_data.append(prepared_product)\n",
    "        \n",
    "    return prepared_data\n",
    "\n",
    "def process_batch_with_claude(products_batch: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Process a batch of products using Claude API with token tracking\"\"\"\n",
    "    global total_input_tokens, total_output_tokens, api_calls_count\n",
    "    \n",
    "    try:\n",
    "        # Prepare the data\n",
    "        prepared_data = prepare_product_data(products_batch)\n",
    "        \n",
    "        # Create the user prompt\n",
    "        user_prompt = f\"\"\"Please process the following product data according to the system instructions:\n",
    "\n",
    "{json.dumps(prepared_data, indent=2, ensure_ascii=False)}\n",
    "\n",
    "Return only the processed JSON array with the specified structure.\"\"\"\n",
    "\n",
    "        logger.info(f\"Processing batch of {len(products_batch)} products...\")\n",
    "        logger.info(f\"Input data size: {len(json.dumps(prepared_data))} characters\")\n",
    "\n",
    "        # Make API call\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=\"claude-haiku-4-5\",\n",
    "            max_tokens=8000,\n",
    "            temperature=0.1,\n",
    "            system=SYSTEM_PROMPT,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Track usage\n",
    "        api_calls_count += 1\n",
    "        input_tokens = response.usage.input_tokens\n",
    "        output_tokens = response.usage.output_tokens\n",
    "        total_input_tokens += input_tokens\n",
    "        total_output_tokens += output_tokens\n",
    "        \n",
    "        # Log API call details\n",
    "        logger.info(f\"API Call #{api_calls_count} completed in {end_time - start_time:.2f}s\")\n",
    "        logger.info(f\"Input tokens: {input_tokens}\")\n",
    "        logger.info(f\"Output tokens: {output_tokens}\")\n",
    "        logger.info(f\"Total tokens used so far: {total_input_tokens + total_output_tokens}\")\n",
    "        \n",
    "        # Parse response\n",
    "        response_text = response.content[0].text.strip()\n",
    "        \n",
    "        # Extract JSON from response (remove any markdown formatting)\n",
    "        if response_text.startswith('```json'):\n",
    "            response_text = response_text[7:]\n",
    "        if response_text.endswith('```'):\n",
    "            response_text = response_text[:-3]\n",
    "        \n",
    "        try:\n",
    "            processed_data = json.loads(response_text)\n",
    "            logger.info(f\"Successfully processed {len(processed_data)} products in this batch\")\n",
    "            return processed_data\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Failed to parse JSON response: {e}\")\n",
    "            logger.error(f\"Raw response: {response_text}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing batch: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Processing functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e34b1f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading product data...\n",
      "‚úÖ Loaded 1 products\n",
      "\n",
      "üìã Example input product:\n",
      "{\n",
      "  \"id\": \"68f9e17b5dc1e9728a133c05\",\n",
      "  \"categories\": [\n",
      "    {\n",
      "      \"categoryId\": 10166,\n",
      "      \"categoryName\": \"Women's Clothing\"\n",
      "    },\n",
      "    {\n",
      "      \"categoryId\": 127386001,\n",
      "      \"categoryName\": \"women's sweater\"\n",
      "    }\n",
      "  ],\n",
      "  \"name\": \"New Autumn and Winter Pullover Knitwear Women's Bottoming Shirt 2025Bf Loose Lazy Style Hong Kong Style Jacquard Quilted Knitwear Women\",\n",
      "  \"Product info\": {\n",
      "    \"length\": null,\n",
      "    \"weight\": null,\n",
      "    \"height\": null,\n",
      "    \"width\": null,\n",
      "    \"aiWeight\": null\n",
      "  },\n",
      "  \"skus\": [\n",
      "    {\n",
      "      \"skuId\": 4712603881561,\n",
      "      \"skuAttributes\": [\n",
      "        {\n",
      "          \"attributeId\": 3216,\n",
      "          \"attributeName\": \"Color\",\n",
      "          \"value\": \"Beige + Orange\"\n",
      "        },\n",
      "        {\n",
      "          \"attributeId\": 450,\n",
      "          \"attributeName\": \"Size\",\n",
      "          \"value\": \"All yards...\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "print(\"üìÇ Loading product data...\")\n",
    "\n",
    "try:\n",
    "    with open('preprocessed.json', 'r', encoding='utf-8') as f:\n",
    "        products = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(products)} products\")\n",
    "    \n",
    "    # Show first product as example\n",
    "    if products:\n",
    "        print(\"\\nüìã Example input product:\")\n",
    "        print(json.dumps(products[0], indent=2, ensure_ascii=False)[:800] + \"...\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File 'preprocessed.json' not found.\")\n",
    "    print(\"Make sure you've run the previous cell that generates this file.\")\n",
    "    products = []\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    products = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e34a5233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 14:40:21,576 - INFO - Processing batch of 1 products...\n",
      "2026-01-09 14:40:21,577 - INFO - Input data size: 937 characters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing single product...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 14:40:22,981 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2026-01-09 14:40:22,984 - INFO - API Call #1 completed in 1.40s\n",
      "2026-01-09 14:40:22,985 - INFO - Input tokens: 1661\n",
      "2026-01-09 14:40:22,987 - INFO - Output tokens: 161\n",
      "2026-01-09 14:40:22,988 - INFO - Total tokens used so far: 1822\n",
      "2026-01-09 14:40:22,989 - INFO - Successfully processed 1 products in this batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Product processed successfully!\n",
      "üíæ Results saved to: apiResponse.json\n",
      "\n",
      "üìä PROCESSING STATISTICS:\n",
      "==================================================\n",
      "üî¢ API calls: 1\n",
      "üì• Input tokens: 1661\n",
      "üì§ Output tokens: 161\n",
      "üîÑ Total tokens: 1822\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Simplified processing for single product or small datasets\n",
    "if not products:\n",
    "    print(\"‚ùå No products to process.\")\n",
    "elif len(products) == 1:\n",
    "    print(\"üîÑ Processing single product...\")\n",
    "    \n",
    "    # Process the single product\n",
    "    processed_result = process_batch_with_claude(products)\n",
    "    \n",
    "    if processed_result:\n",
    "        print(\"‚úÖ Product processed successfully!\")\n",
    "        \n",
    "        # Save results\n",
    "        output_filename = f\"apiResponse.json\"\n",
    "        \n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_result, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Results saved to: {output_filename}\")\n",
    "        \n",
    "        # Show processing statistics\n",
    "        print(f\"\\nüìä PROCESSING STATISTICS:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üî¢ API calls: {api_calls_count}\")\n",
    "        print(f\"üì• Input tokens: {total_input_tokens}\")\n",
    "        print(f\"üì§ Output tokens: {total_output_tokens}\")\n",
    "        print(f\"üîÑ Total tokens: {total_input_tokens + total_output_tokens}\")\n",
    "        print(\"=\" * 50)\n",
    "    else:\n",
    "        print(\"‚ùå Failed to process product\")\n",
    "else:\n",
    "    print(f\"üöÄ Processing {len(products)} products...\")\n",
    "    # Use the batch processing code for multiple products\n",
    "    print(\"üìù Multiple products detected - use the batch processing cell above instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9a372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d19d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f9fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2348746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf9d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
